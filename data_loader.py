import os
import pandas as pd
from sklearn.model_selection import train_test_split
import glob
import random

# --- Configuration ---
# These paths would need to be adjusted for your actual data structure
TCIA_DATA_DIR = "path/to/your/tcia_data"
LOCAL_DATA_DIR = "path/to/your/local_data"
METADATA_FILE = "path/to/your/metadata.csv" # CSV with PatientID, PTEN_Mutation (0 or 1), DataSource (TCIA/Local)

# Define expected file patterns (adjust as needed)
# Assumes files are named like: PatientID_T1w.nii.gz, PatientID_T1c.nii.gz, etc.
MODALITIES = ["T1w", "T1c", "T2w", "FLAIR"]
SEG_SUFFIX_WT = "_seg_WT.nii.gz" # Example suffix for Whole Tumor mask
SEG_SUFFIX_TC = "_seg_TC.nii.gz" # Example suffix for Tumor Core mask
# --- ---

def find_patient_files(patient_id, data_source):
    """Finds all required MRI and segmentation files for a patient."""
    base_dir = TCIA_DATA_DIR if data_source == "TCIA" else LOCAL_DATA_DIR
    patient_files = {'id': patient_id, 'source': data_source}
    valid = True
    for mod in MODALITIES:
        # Use glob to handle potential variations if exact naming isn't guaranteed
        pattern = os.path.join(base_dir, patient_id, f"{patient_id}*{mod}*.nii*") # More flexible pattern
        found_files = glob.glob(pattern)
        if not found_files:
             # Try alternative common naming
             pattern = os.path.join(base_dir, f"{patient_id}*{mod}*.nii*")
             found_files = glob.glob(pattern)

        if len(found_files) == 1:
             patient_files[mod] = found_files[0]
        elif len(found_files) > 1:
             print(f"Warning: Found multiple {mod} files for {patient_id}. Using first one: {found_files[0]}")
             patient_files[mod] = found_files[0]
        else:
             print(f"Warning: Missing {mod} file for patient {patient_id}")
             valid = False
             break # Stop if a modality is missing

    if not valid: return None

    # Find segmentation files (assuming they exist, generated by segmentation step)
    # Adjust path/naming as needed based on where segmentation outputs are stored
    wt_path = os.path.join(base_dir, patient_id, f"{patient_id}{SEG_SUFFIX_WT}")
    tc_path = os.path.join(base_dir, patient_id, f"{patient_id}{SEG_SUFFIX_TC}")

    if not os.path.exists(wt_path): # Try alternative location/naming if needed
        wt_path = os.path.join(base_dir, f"{patient_id}{SEG_SUFFIX_WT}") # Outside patient folder?
    if not os.path.exists(tc_path):
        tc_path = os.path.join(base_dir, f"{patient_id}{SEG_SUFFIX_TC}")

    if os.path.exists(wt_path):
        patient_files['seg_WT'] = wt_path
    else:
        print(f"Warning: Missing WT segmentation for patient {patient_id}")
        # Decide how to handle missing segmentations (skip patient, etc.)
        # For now, we might allow proceeding for CNN (uses WT) but fail radiomics
        # valid = False # Uncomment to skip patients missing WT seg
        patient_files['seg_WT'] = None # Mark as missing

    if os.path.exists(tc_path):
         patient_files['seg_TC'] = tc_path
    else:
         print(f"Warning: Missing TC segmentation for patient {patient_id}")
         patient_files['seg_TC'] = None # Mark as missing
         # TC is used in radiomics, might need to skip radiomics for this patient if missing

    return patient_files if valid else None


def load_data_manifest(metadata_path=METADATA_FILE):
    """Loads patient metadata and finds corresponding files."""
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

    metadata = pd.read_csv(metadata_path)
    # Expect columns: PatientID, PTEN_Mutation (0=WildType, 1=Mutated), DataSource (TCIA/Local)
    # Add more columns if needed (e.g., Age, Sex from Table 1 for analysis)

    all_patient_data = []
    for _, row in metadata.iterrows():
        patient_id = str(row['PatientID']) # Ensure string
        data_source = row['DataSource']
        files = find_patient_files(patient_id, data_source)
        if files:
            files['label'] = int(row['PTEN_Mutation'])
            # Add other metadata if needed
            # files['age'] = row['Age']
            # files['sex'] = row['Sex']
            all_patient_data.append(files)
        else:
            print(f"Skipping patient {patient_id} due to missing files.")

    return all_patient_data

def split_data(all_patient_data, test_size=0.3, random_state=42):
    """Splits the list of patient data dictionaries into training and validation sets."""
    # Paper split: 170 training, 74 validation (~30.3% validation)
    if not all_patient_data:
        raise ValueError("No patient data loaded to split.")

    patient_ids = [p['id'] for p in all_patient_data]
    labels = [p['label'] for p in all_patient_data]

    train_ids, val_ids = train_test_split(
        patient_ids,
        test_size=test_size,
        random_state=random_state,
        stratify=labels # Stratify to keep label distribution similar
    )

    train_data = [p for p in all_patient_data if p['id'] in train_ids]
    val_data = [p for p in all_patient_data if p['id'] in val_ids]

    print(f"Data split: {len(train_data)} training, {len(val_data)} validation samples.")
    # Optional: Print label distribution in splits
    train_labels = [p['label'] for p in train_data]
    val_labels = [p['label'] for p in val_data]
    print(f"Training labels: {sum(train_labels)} mutated / {len(train_labels)}")
    print(f"Validation labels: {sum(val_labels)} mutated / {len(val_labels)}")


    return train_data, val_data

# Example usage (in main.py):
# from data_loader import load_data_manifest, split_data
# all_data = load_data_manifest()
# train_list, val_list = split_data(all_data)